<!DOCTYPE html>
<!-- saved from url=(0042)https://sagarverma.github.io/dynamics.html -->
<html lang="en-gb" dir="ltr" class="com_content view-article layout-blog itemid-118 j39 mm-hover  no-touch"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

	<meta name="keywords" content="computer vision, research">
	<meta name="author" content="Ram Sharma">
	<meta name="description" content="Centre for Visual Information Technology (CVIT) is a research centre at International Institute of Information Technology, Hyderabad.">
	<meta name="generator" content="Joomla! - Open Source Content Management">
	<title>Concept Drift Detection for Multivariate Data Streams and
Temporal Segmentation of Daylong Egocentric Videos</title>
	<link href="./stylesandscripts/bootstrap.css" rel="stylesheet" type="text/css">
	<link href="./stylesandscripts/system.css" rel="stylesheet" type="text/css">
	<link href="./stylesandscripts/template.css" rel="stylesheet" type="text/css">
	<link href="./stylesandscripts/megamenu.css" rel="stylesheet" type="text/css">
	<link href="./stylesandscripts/font-awesome.min.css" rel="stylesheet" type="text/css">
	<link href="./stylesandscripts/magazine.css" rel="stylesheet" type="text/css">
	<script src="./stylesandscripts/jquery.min.js" type="text/javascript"></script>
	<script src="./stylesandscripts/jquery-noconflict.js" type="text/javascript"></script>
	<script src="./stylesandscripts/jquery-migrate.min.js" type="text/javascript"></script>
	<script src="./stylesandscripts/caption.js" type="text/javascript"></script>
	<script src="./stylesandscripts/bootstrap.js" type="text/javascript"></script>
	<script src="./stylesandscripts/jquery.tap.min.js" type="text/javascript"></script>
	<script src="./stylesandscripts/script.js" type="text/javascript"></script>
	<script src="./stylesandscripts/menu.js" type="text/javascript"></script>
	<script src="./stylesandscripts/nav-collapse.js" type="text/javascript"></script>
	<script type="text/javascript">

		jQuery(window).on('load',  function() {
				new JCaption('img.caption');
			});

		jQuery(function($){ initTooltips(); $("body").on("subform-row-add", initTooltips); function initTooltips (event, container) { container = container || document;$(container).find(".hasTooltip").tooltip({"html": true,"container": "body"});} });
	</script>

	<style type="text/css">

		.card { display: inline-block; vertical-align: top; width: 500px; height: 150px; position: relative; overflow: hidden; margin: 20px; background: #FFF; box-shadow: 0 2px 5px 0 rgba(0, 0, 0, 0.16), 0 2px 10px 0 rgba(0, 0, 0, 0.12); color: #272727; border-radius: 2px;}


		.card .image { position: relative; height: 150px; width: 150px; float: left;}


		.card .image img { border-radius: 2px 0 0 2px; position: relative; left: 0; right: 0; top: 0; bottom: 0; height: 100%; -moz-transition:-moz-transform 0.5s ease-in; -webkit-transition:-webkit-transform 0.5s ease-in; -o-transition:-o-transform 0.5s ease-in; transition: 0.5s ease-in;}

		.card .image img:hover{ -moz-transform:scale(.95); -webkit-transform:scale(.95); -o-transform:scale(.95); transform:scale(.95);}/* .card img:hover{ -webkit-transform: rotateZ(-10deg); -ms-transform: rotateZ(-10deg); transform: rotateZ(-10deg); transition: 1s ease;}*/

		.card .title { padding-top: 10px !important; line-height: 24px; font-size: 18px; font-weight: 600; position: relative; top: 10px; padding: 20px; font-variant: small-caps;}


		.card .content { font-size: 13; width: 350px; float: left; padding: 20px; font-weight: 300; border-radius: 0 2px 2px 0;}.card p { margin: 0;}


		.card a { font-size: 13px; color: #29739D; margin-right: 20px; -webkit-transition: color 0.3s ease; transition: color 0.3s ease; text-transform: uppercase; text-decoration: none;}

		.card .card-footer { text-align: right; width: 350px; position: absolute; bottom: 0; left: 150px; border-top: 1px solid rgba(160, 160, 160, 0.2); margin: 0; padding-left: 20px; padding-right: 10px;}

		.card .card-footer a { color: #ffab40; margin-right: 20px; -webkit-transition: color 0.3s ease; transition: color 0.3s ease; text-transform: uppercase; text-decoration: none;}

		.recents { font-size: 16px; padding: 7px 15px; background-color: #eeeeee;}

		.recent-updates-module { height: 400px; overflow: auto; text-align: justify;}</style>


<!-- META FOR IOS & HANDHELD -->
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">


	<style type="text/stylesheet">
		@-webkit-viewport   { width: device-width; }
		@-moz-viewport      { width: device-width; }
		@-ms-viewport       { width: device-width; }
		@-o-viewport        { width: device-width; }
		@viewport           { width: device-width; }
	</style>


	<script type="text/javascript">
		//<![CDATA[
		if (navigator.userAgent.match(/IEMobile\/10\.0/)) {
			var msViewportStyle = document.createElement("style");
			msViewportStyle.appendChild(
				document.createTextNode("@-ms-viewport{width:auto!important}")
			);
			document.getElementsByTagName("head")[0].appendChild(msViewportStyle);
		}

	</script>


<meta name="HandheldFriendly" content="true">
<meta name="apple-mobile-web-app-capable" content="YES">
<!-- //META FOR IOS & HANDHELD -->




<!-- Le HTML5 shim and media query for IE8 support -->
<!--[if lt IE 9]>
<script src="//html5shim.googlecode.com/svn/trunk/html5.js"></script>
<script type="text/javascript" src="/plugins/system/t3/base-bs3/js/respond.min.js"></script>
<![endif]-->

<!-- You can add Google Analytics here or use T3 Injection feature -->

<!--[if lt IE 9]>
<link rel="stylesheet" href="/templates/purity_iii/css/ie8.css" type="text/css" />
<![endif]-->	</head>

<body data-gr-c-s-loaded="true">

<div class="t3-wrapper magazine"> <!-- Need this wrapper for off-canvas menu. Remove if you don't use of-canvas -->


<div id="t3-mainbody" class="container t3-mainbody">
	<div class="row">

		<!-- MAIN CONTENT -->
		<div id="t3-content" class="t3-content col-xs-12">


<div class="item-page clearfix">


<!-- Article -->
<article itemscope="" itemtype="http://schema.org/Article">
	<meta itemprop="inLanguage" content="en-GB">
	<meta itemprop="url" content="/research/projects/cvit-projects/badminton-analytics">

	<section class="article-content clearfix" itemprop="articleBody">
		<h3 style="text-align: center; color:#000000"><strong><u>Concept Drift Detection for Multivariate Data Streams and
Temporal Segmentation of Daylong Egocentric Videos</u></strong></h3><br>
		<div class="row">
				<center>
		          <div class="col-md-8 no-pad">
		            <div class="authors">

		              <a target="blank" href="https://pravin74.github.io/">Pravin Nagar </a>,
		              <a target="blank" href="#">    Mansi Khemka    </a>,
									<a target="blank" href="https://www.cse.iitd.ac.in/~chetan/">    Chetan Arora</a>

		            </div>
		          </div>
    </center>
    </div>
<hr>
<h3 style="color: #0e083b"><b>Abstract</b></h3>
<div style="text-align: justify;color: #083e40">The long and unconstrained nature of egocentric videos makes it imperative to use temporal segmentation as an important pre-processing step for many higher-level inference tasks. Activities of the wearer in an egocentric video typically span over hours and are often separated by slow, gradual changes. Furthermore, the change of camera viewpoint due to the wearer's head motion causes frequent and extreme, but, spurious scene changes. The continuous nature of boundaries makes it difficult to apply traditional Markov Random Field (MRF) pipelines relying on temporal discontinuity, whereas deep Long Short Term Memory (LSTM) networks gather context only upto a few hundred frames, rendering them ineffective for egocentric videos. In this paper, we present a novel unsupervised temporal segmentation technique especially suited for day-long egocentric videos. We formulate the problem as detecting concept drift in a time-varying, non i.i.d. sequence of frames.  Statistically bounded thresholds are calculated to detect concept drift between two temporally adjacent multivariate data segments with different underlying distributions while establishing guarantees on false positives. Since the derived threshold indicates confidence in the prediction, it can also be used to control the granularity of the output segmentation. Using our technique, we report significantly improved state of the art f-measure for daylong egocentric video datasets, as well as photostream datasets derived from them: HUJI~(73.01%, 59.44%), UTEgo~(58.41%, $60.61%) and Disney~(67.63%, 68.83%). </div>
<p>&nbsp;</p>

<h3 style="color: #0e083b"><b>Methodology</b></h3><br>
<div><img style="display: block; margin-left: auto; margin-right: auto;border: solid 2px black; width: 100%" src="Architecture.png" alt="IMG"><center><small></small></center></div>
<br><br>
<h3 style="color: #0e083b"><b>Visualization</b></h3><br>
<div style="text-align: justify;color: #083e40">The figure shows a qualitative representation of closeness of boundaries predicted by the proposed approach, ADaptive WINdowing (ADWIN), Contextual Event Segmentation (CES) \cite{del2018predicting} to ground truth boundaries from specific portions of Huji (first row), UTEgo (second row), and Disney (third row) datasets. </div>
<br>
<div><img style="display: block; margin-left: auto; margin-right: auto;border: solid 2px black;width: 100%" src="visualization.png" alt="IMG"><center><small></small></center></div>
<hr>
<!-- <br>
<h3 style="color: #0e083b"><b>User Study</b></h3><br>
<div style="text-align: justify;color: #083e40">The table shows the Likert score of 1 (Extremely dissatisfied) to 5 (Extremely satisfied) given by the participants when specific events are included or excluded in the summary with user comments on the personalized summary. S0X-SY represents subject ‘X’ in scenario ‘Y’. It is observed that sometimes the user sees the excluded part in the personalized summary. This is because the interactive reward personalized the summary but at the same time distinctiveness-indicative reward that tries to maintain the global context. This can be handled by fine-tuning the weights of A and B discussed in interactive reward.</div> -->
<!-- <br>
 <iframe src="Interactive_user_feedback.pdf" width="100%" height="500px">
 </iframe> -->

<!-- <h3 style="color: #0e083b"><b>Related Publications</b></h3>
<ul>
<li>Anuj Rathore*, Pravin Nagar* , Chetan Arora, and C. V. Jawahar. "Generating 1 Minute Summaries of Day Long Egocentric Videos" ACMMM 2019.</li>
</ul>
<hr> -->
<h3>Bibtex</h3>
<p>If you use this work, please cite this work also :</p>
<div>
<pre style="border-style: solid; border-width: 1px;"> @article{nagarconcept,
  title={Concept Drift Detection for Multivariate Data Streams and Temporal Segmentation of Daylong Egocentric Videos},
  author={Nagar, Pravin and Khemka, Mansi and Arora, Chetan}
}
</pre>
</div>
<hr>
<div>
<h3 style="color: #0e083b"><b>Code</b></h3>
<p>The software implementation of this project can be found on <a href="https://github.com/Pravin74/temporal_video_segmentation_code">GitHub repository</a>. Find the <a href="https://drive.google.com/drive/folders/1L0I9wZhYnh2xV8e8A3QFklYU5pAk8F5l?usp=sharing">Features</a></p>
</div>
<hr>
<div>
<h3 style="color: #0e083b"><b>Supplementary Material</b></h3>
<p>The supplementary material can be found<a href="supplementary.pdf"> here </a>. Find the <a href="https://www.youtube.com/watch?v=tHxj_bR73Rs">Demo</a></p>
</div>

<hr>
<div>
<!-- <h3 style="color: #0e083b"><b>Dataset</b></h3>
<div>
<p>The Feature extraction process and feature loading from h5py files is explained in the <a href="ReadMe_dataset_preparation.pdf"> ReadMe. </a></p>
</div>
<div class="row">

	<div class="col-xs-3">
		<li style="padding-left: 50%"><a href="http://ai.stanford.edu/~alireza/Disney/">DISNEY: </a></p></li>
		<li style="padding-left: 50%"><a href="http://vision.cs.utexas.edu/projects/egocentric_data/UT_Egocentric_Dataset.html">UTE: </a></p></li>
		<li style="padding-left: 50%"><a href="http://www.vision.huji.ac.il/egoseg/videos/dataset.html">HUJI: </a></p></li>

	</div>

	<div class="col-xs-3">
		<ul><a href="https://drive.google.com/drive/folders/1-Q-ur3TAfQi-WIfkAYn-dG3TUpdnR_K6">CNN Features</a></p></ul>
		<ul><a href="#">CNN Features</a></p></ul>
		<ul><a href="#">CNN Features</a></p></ul>
</div>
<div class="col-xs-3">
	<ul><a href="https://drive.google.com/drive/folders/1-Q-ur3TAfQi-WIfkAYn-dG3TUpdnR_K6">C3D Features</a></p></ul>
	<ul><a href="https://drive.google.com/drive/folders/1FswxUAgKBhx02pUNv610-kG7lbE-PJxu">C3D Features</a></p></ul>
	<ul><a href="https://drive.google.com/drive/folders/199UmwjLnqTGLSo7QgaPsaYFAAxieqSLY">C3D Features</a></p></ul>
	</div>
<div class="col-xs-3">
	<ul><a href="http://ai.stanford.edu/~syyeung/videoset.html">Annotations</a></p></ul>
	<ul><a href="">Annotations(ours)</a></p></ul>
</div>
</div>


</div>
<hr>
	</section>

	</article>

</div>


		</div>

	</div>
</div>

</div> -->


</body></html>
